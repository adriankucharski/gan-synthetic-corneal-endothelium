{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "save_disk = \"D:/TF_LOGS\"\n",
    "k = 3\n",
    "\n",
    "paths_pix2pix = [\n",
    "    [\"Alizarine\", \"20230323-110848\", 0],\n",
    "    [\"Alizarine\", \"20230327-224625\", 1],\n",
    "    [\"Alizarine\", \"20230328-021427\", 2],\n",
    "    [\"Gavet\", \"20230323-120633\", 0],\n",
    "    [\"Gavet\", \"20230327-234144\", 1],\n",
    "    [\"Gavet\", \"20230328-030749\", 2],\n",
    "    [\"Rotterdam_1000\", \"20230323-132240\", 0],\n",
    "    [\"Rotterdam_1000\", \"20230328-004342\", 1],\n",
    "    [\"Rotterdam_1000\", \"20230328-095439\", 2],\n",
    "]\n",
    "\n",
    "paths_our = [\n",
    "    [\"Alizarine\", \"20220405-2359\", 0],\n",
    "    [\"Alizarine\", \"20220603-1720\", 1],\n",
    "    [\"Alizarine\", \"20220603-1850\", 2],\n",
    "    [\"Gavet\", \"20220429-0021\", 0],\n",
    "    [\"Gavet\", \"20220609-2213\", 1],\n",
    "    [\"Gavet\", \"20220609-2319\", 2],\n",
    "    [\"Rotterdam_1000\", \"20220623-0916\", 0],\n",
    "    [\"Rotterdam_1000\", \"20220609-1841\", 1],\n",
    "    [\"Rotterdam_1000\", \"20220609-2031\", 2],\n",
    "    [\"Rotterdam_1000\", \"20230330-1333\", 1],\n",
    "]\n",
    "\n",
    "for dataset_name, date_path, fold_id in paths_pix2pix:\n",
    "    save_path = os.path.join(save_disk, f\"pix2pix_fid/{dataset_name}_{date_path}_{fold_id}.npy\")\n",
    "    arr = np.load(save_path)\n",
    "    print(dataset_name, date_path, fold_id, np.sort(arr)[:k], np.argsort(arr)[:k])\n",
    "    # print(list(zip(np.argsort(arr)[:k], np.sort(arr)[:k])))\n",
    "    # for i in np.argsort(arr)[:k]:\n",
    "    #     g_path = f\"F:/TF_LOGS/{dataset_name}/fit/{date_path}/generators/gan_{i}.hdf5\"\n",
    "    #     print(g_path)\n",
    "\n",
    "for dataset_name, date_path, fold_id in paths_our:\n",
    "    save_path = os.path.join(save_disk, f\"our_fid/{dataset_name}_{date_path}_{fold_id}.npy\")\n",
    "    if not os.path.exists(save_path):\n",
    "        continue\n",
    "    arr = np.load(save_path)\n",
    "    print(dataset_name, date_path, fold_id, np.sort(arr)[:k], np.argsort(arr)[:k])\n",
    "    # print(list(zip(np.argsort(arr)[:k], np.sort(arr)[:k])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pix2pix approach - generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DataIterator, load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from keras import Model, Input, Sequential\n",
    "from keras.models import load_model\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "paths = [\n",
    "    [\"Alizarine\", \"20230323-110848\", 0],\n",
    "    [\"Alizarine\", \"20230327-224625\", 1],\n",
    "    [\"Alizarine\", \"20230328-021427\", 2],\n",
    "    [\"Gavet\", \"20230323-120633\", 0],\n",
    "    [\"Gavet\", \"20230327-234144\", 1],\n",
    "    [\"Gavet\", \"20230328-030749\", 2],\n",
    "    [\"Rotterdam_1000\", \"20230323-132240\", 0],\n",
    "    [\"Rotterdam_1000\", \"20230328-004342\", 1],\n",
    "    [\"Rotterdam_1000\", \"20230328-095439\", 2],\n",
    "]\n",
    "\n",
    "save_disk = \"D:/\"\n",
    "\n",
    "for dataset_name, date_path, fold_id in paths:\n",
    "    data_path = f\"./datasets/{dataset_name}/folds.json\"\n",
    "    train, test = load_dataset(data_path, True, as_numpy=False)[fold_id]\n",
    "    dataset_mask = DataIterator(\n",
    "        train, 32, patch_per_image=32, inv_values=False\n",
    "    ).get_dataset()\n",
    "    dataset_test = DataIterator(\n",
    "        test, 32, patch_per_image=64, inv_values=False\n",
    "    ).get_dataset()\n",
    "\n",
    "    images, masks = dataset_test[1], dataset_mask[0]\n",
    "    validation_data = np.asarray(\n",
    "        [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in images], dtype=np.uint8\n",
    "    )\n",
    "\n",
    "    for k in range(len(validation_data)):\n",
    "        fp = os.path.join(\n",
    "            save_disk, f\"TF_LOGS/pix2pix/{dataset_name}/dataset_true_fold_{fold_id}/\"\n",
    "        )\n",
    "        Path(fp).mkdir(exist_ok=True, parents=True)\n",
    "        io.imsave(\n",
    "            os.path.join(fp, f\"{k}.png\"), validation_data[k], check_contrast=False\n",
    "        )\n",
    "\n",
    "    for i in tqdm(range(1, 101)):\n",
    "        g_path = f\"F:/TF_LOGS/{dataset_name}/fit/{date_path}/generators/gan_{i}.hdf5\"\n",
    "        generator: Model = load_model(g_path)\n",
    "        new_mask = generator.predict_on_batch(masks)\n",
    "        imgs = np.asarray(\n",
    "            [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in new_mask],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        for k in range(len(imgs)):\n",
    "            fp = os.path.join(\n",
    "                save_disk, f\"TF_LOGS/pix2pix/{dataset_name}/{date_path}/{i}/\"\n",
    "            )\n",
    "            Path(fp).mkdir(exist_ok=True, parents=True)\n",
    "            io.imsave(os.path.join(fp, f\"{k}.png\"), imgs[k], check_contrast=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our approach - generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DataIterator, load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from keras import Model, Input, Sequential\n",
    "from keras.models import load_model\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def remove_gaussian_from_model(model: Model) -> Model:\n",
    "    lay_1 = model.get_layer(\"conv2d_7\")\n",
    "    lay_2 = model.get_layer(\"output\")\n",
    "    x = lay_1(model.layers[-4].output)\n",
    "    x = lay_2(x)\n",
    "    new_model = Model(inputs=model.input, outputs=x)\n",
    "    return new_model\n",
    "\n",
    "\n",
    "paths = [\n",
    "    [\"Rotterdam_1000\", \"20230330-1333\", 1],\n",
    "    # [\"Rotterdam_1000\", \"20220623-0916\", 0],\n",
    "    # [\"Rotterdam_1000\", \"20220609-1841\", 1],\n",
    "    # [\"Rotterdam_1000\", \"20220609-2031\", 2],\n",
    "    # [\"Alizarine\", \"20220405-2359\", 0],\n",
    "    # [\"Alizarine\", \"20220603-1720\", 1],\n",
    "    # [\"Alizarine\", \"20220603-1850\", 2],\n",
    "    # [\"Gavet\", \"20220429-0021\", 0],\n",
    "    # [\"Gavet\", \"20220609-2213\", 1],\n",
    "    # [\"Gavet\", \"20220609-2319\", 2],\n",
    "]\n",
    "\n",
    "save_disk = \"R:/\"\n",
    "save_true = False\n",
    "\n",
    "for dataset_name, date_path, fold_id in paths:\n",
    "    data_path = f\"./datasets/{dataset_name}/folds.json\"\n",
    "    train, test = load_dataset(data_path, True, as_numpy=False)[fold_id]\n",
    "    dataset_mask = DataIterator(\n",
    "        train, 32, patch_per_image=32, inv_values=False\n",
    "    ).get_dataset()\n",
    "    dataset_test = DataIterator(\n",
    "        test, 32, patch_per_image=64, inv_values=False\n",
    "    ).get_dataset()\n",
    "\n",
    "    images, masks = dataset_test[1], dataset_mask[0]\n",
    "\n",
    "    validation_data = np.asarray(\n",
    "        [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in images],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    for k in range(len(validation_data)):\n",
    "        fp = os.path.join(\n",
    "            save_disk, f\"TF_LOGS/our/{dataset_name}/dataset_true_fold_{fold_id}/\"\n",
    "        )\n",
    "        Path(fp).mkdir(exist_ok=True, parents=True)\n",
    "        io.imsave(\n",
    "            os.path.join(fp, f\"{k}.png\"), validation_data[k], check_contrast=False\n",
    "        )\n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "        g_path = (\n",
    "            f\"F:/Deep Learning/GAN-CNN-CE/generator/models/{date_path}/model_{i}.h5\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(g_path):\n",
    "            continue\n",
    "        \n",
    "        generator: Model = remove_gaussian_from_model(load_model(g_path))\n",
    "        new_mask = generator.predict_on_batch([1-masks, np.random.normal(size=masks.shape)])\n",
    "        imgs = np.asarray(\n",
    "            [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in new_mask],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "        for k in range(len(imgs)):\n",
    "            fp = os.path.join(save_disk, f\"TF_LOGS/our/{dataset_name}/{date_path}/{i}/\")\n",
    "            Path(fp).mkdir(exist_ok=True, parents=True)\n",
    "            io.imsave(os.path.join(fp, f\"{k}.png\"), imgs[k], check_contrast=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate and save FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "save_disk = \"R:/TF_LOGS\"\n",
    "paths_pix2pix = [\n",
    "    # [\"Alizarine\", \"20230323-110848\", 0],\n",
    "    # [\"Alizarine\", \"20230327-224625\", 1],\n",
    "    # [\"Alizarine\", \"20230328-021427\", 2],\n",
    "    # [\"Gavet\", \"20230323-120633\", 0],\n",
    "    # [\"Gavet\", \"20230327-234144\", 1],\n",
    "    # [\"Gavet\", \"20230328-030749\", 2],\n",
    "    # [\"Rotterdam_1000\", \"20230323-132240\", 0],\n",
    "    # [\"Rotterdam_1000\", \"20230328-004342\", 1],\n",
    "    # [\"Rotterdam_1000\", \"20230328-095439\", 2],\n",
    "]\n",
    "\n",
    "paths_our = [\n",
    "    [\"Rotterdam_1000\", \"20230330-1333\", 1],\n",
    "    # [\"Rotterdam_1000\", \"20220623-0916\", 0],\n",
    "    # [\"Rotterdam_1000\", \"20220609-1841\", 1],\n",
    "    # [\"Rotterdam_1000\", \"20220609-2031\", 2],\n",
    "    # [\"Alizarine\", \"20220405-2359\", 0],\n",
    "    # [\"Alizarine\", \"20220603-1720\", 1],\n",
    "    # [\"Alizarine\", \"20220603-1850\", 2],\n",
    "    # [\"Gavet\", \"20220429-0021\", 0],\n",
    "    # [\"Gavet\", \"20220609-2213\", 1],\n",
    "    # [\"Gavet\", \"20220609-2319\", 2],\n",
    "]\n",
    "\n",
    "if False:\n",
    "    for dataset_name, date_path, fold_id in paths_pix2pix:\n",
    "        true_path = os.path.join(\n",
    "            save_disk, f\"pix2pix/{dataset_name}/dataset_true_fold_{fold_id}/\"\n",
    "        )\n",
    "        fids = []\n",
    "        for i in tqdm.tqdm(range(1, 101)):\n",
    "            pred_path = os.path.join(save_disk, f\"pix2pix/{dataset_name}/{date_path}/{i}/\")\n",
    "            command = (\n",
    "                f\"python -m pytorch_fid {str(pred_path)} {str(true_path)} --device cuda:0\"\n",
    "            )\n",
    "            arch = subprocess.check_output(command, shell=True)\n",
    "            fid = float(re.findall(\"\\d+\\.\\d+\", str(arch))[0])\n",
    "            fids.append(fid)\n",
    "        save_path = os.path.join(save_disk, f\"{dataset_name}_{date_path}_{fold_id}.npy\")\n",
    "        print(save_path, true_path)\n",
    "        np.save(save_path, np.array(fids))\n",
    "if True:\n",
    "    for dataset_name, date_path, fold_id in paths_our:\n",
    "        true_path = os.path.join(\n",
    "            save_disk, f\"our/{dataset_name}/dataset_true_fold_{fold_id}/\"\n",
    "        )\n",
    "        fids = []\n",
    "        for i in tqdm.tqdm(range(100)):\n",
    "            pred_path = os.path.join(save_disk, f\"our/{dataset_name}/{date_path}/{i}/\")\n",
    "            command = (\n",
    "                f\"python -m pytorch_fid {str(pred_path)} {str(true_path)} --device cuda:0\"\n",
    "            )\n",
    "            arch = subprocess.check_output(command, shell=True)\n",
    "            fid = float(re.findall(\"\\d+\\.\\d+\", str(arch))[0])\n",
    "            fids.append(fid)\n",
    "        save_path = os.path.join(save_disk, f\"our_fid/{dataset_name}_{date_path}_{fold_id}.npy\")\n",
    "        print(save_path, true_path)\n",
    "        np.save(save_path, np.array(fids))\n",
    "\n",
    "# os.system(\"shutdown /s /t 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('R:/Rotterdam_1000_20230330-1333_1.npy', fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "save_disk = \"D:/TF_LOGS\"\n",
    "k = 4\n",
    "\n",
    "paths_pix2pix = [\n",
    "    [\"Alizarine\", \"20230323-110848\", 0, '20230329-2116', 7],\n",
    "    [\"Alizarine\", \"20230327-224625\", 1, '20230329-2120', 7],\n",
    "    [\"Alizarine\", \"20230328-021427\", 2, '20230329-2123', 7],\n",
    "    [\"Gavet\", \"20230323-120633\", 0, '20230329-2127', 9],\n",
    "    [\"Gavet\", \"20230327-234144\", 1, '20230329-2131', 9],\n",
    "    [\"Gavet\", \"20230328-030749\", 2, '20230329-2135', 9],\n",
    "    [\"Rotterdam_1000\", \"20230323-132240\", 0, '20230329-2139', 15],\n",
    "    [\"Rotterdam_1000\", \"20230328-004342\", 1, '20230329-2143', 15],\n",
    "    [\"Rotterdam_1000\", \"20230328-095439\", 2, '20230329-2146', 15],\n",
    "]\n",
    "\n",
    "results = []\n",
    "for dataset_name, date_path, fold_id, models_p, wsize in tqdm(paths_pix2pix):\n",
    "    command = (\n",
    "        f\"python evaluate.py {dataset_name} {fold_id} segmentation/models/synthetic_pix2pix/{models_p} {wsize}\"\n",
    "    )\n",
    "    arch = subprocess.check_output(command, shell=True)\n",
    "    results.append(arch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Benchmark\n",
    "\n",
    "#### 32x32x3\n",
    "##### Batch size: 512\n",
    "PL 100% - 5.84s\n",
    "PL  90% - 5.85s\n",
    "PL  80% - 5.85s\n",
    "PL  70% - 5.89s\n",
    "PL  60% - 6.07s\n",
    "PL  50% - 6.20s\n",
    "##### Batch size: 2048\n",
    "PL 100% - 3.97s\n",
    "PL  90% - 3.98s\n",
    "PL  80% - 4.14s\n",
    "PL  70% - 3.78s\n",
    "PL  60% - 3.80s\n",
    "PL  50% - 4.12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:11<03:03,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002740A208940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:11<02:26,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002740A20A320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230412-1813 [107.47449734 117.01175454 126.55006418 131.62885976 131.72459966] [ 5 32 22 31  7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230412-1829 [ 79.45859402 107.89416214 116.46304104 116.94744639 121.08492495] [47 49 11 12 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:46<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230412-1845 [ 93.99914365 126.15721594 130.16915836 138.72851774 143.11731833] [44 38 37 12 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230412-1904 [63.7617624  76.76399027 84.39626781 92.12829262 97.53652036] [39 32 34 19 27]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from dataset import DataIterator, load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from keras import Model, Input, Sequential\n",
    "from keras.models import load_model\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "import subprocess\n",
    "import re\n",
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def remove_gaussian_from_model(model: Model) -> Model:\n",
    "    lay_1 = model.get_layer(index=-2)\n",
    "    lay_2 = model.get_layer(\"output\")\n",
    "    x = lay_1(model.layers[-4].output)\n",
    "    x = lay_2(x)\n",
    "    new_model = Model(inputs=model.input, outputs=x)\n",
    "    return new_model\n",
    "\n",
    "\n",
    "paths = [\n",
    "\n",
    "    [\"Gavet\", \"20230412-1813\", 3],\n",
    "    [\"Gavet\", \"20230412-1829\", 3],\n",
    "    [\"Gavet\", \"20230412-1845\", 3],\n",
    "    [\"Gavet\", \"20230412-1904\", 3],\n",
    "    # [\"Alizarine\", \"20230411-2300\", 3],\n",
    "    # [\"Alizarine\", \"20230410-2152\", 3],\n",
    "    # [\"Alizarine\", \"20230410-2229\", 3],\n",
    "    # [\"Alizarine\", \"20230410-2306\", 3],\n",
    "    # [\"Alizarine\", \"20230410-2338\", 3],\n",
    "    # [\"Alizarine\", \"20230411-0013\", 3],\n",
    "    \n",
    "    # [\"Rotterdam_1000\", \"20230408-0926\", 3],\n",
    "    # [\"Rotterdam_1000\", \"20230408-1025\", 3],\n",
    "    # [\"Rotterdam_1000\", \"20230408-1124\", 3],\n",
    "    # [\"Rotterdam_1000\", \"20230408-1226\", 3],\n",
    "    # [\"Rotterdam_1000\", \"20230408-1318\", 3],\n",
    "    # [\"Rotterdam_1000\", \"20230408-1413\", 3],\n",
    "\n",
    "    \n",
    "    # [\"Gavet\", \"20230407-1804\", 3],\n",
    "    # [\"Gavet\", \"20230407-1840\", 3],\n",
    "    # [\"Gavet\", \"20230407-1915\", 3],\n",
    "    # [\"Gavet\", \"20230407-1955\", 3],\n",
    "    # [\"Gavet\", \"20230407-2028\", 3],\n",
    "    # [\"Gavet\", \"20230407-2104\", 3],\n",
    "\n",
    "    \n",
    "    # [\"Gavet\", \"20220609-2213\", 1],\n",
    "    # [\"Gavet\", \"20230404-1136\", 1],\n",
    "    # [\"Gavet\", \"20230406-1951\", 1],\n",
    "    # [\"Gavet\", \"20230406-2014\", 1],\n",
    "    # [\"Gavet\", \"20230406-2037\", 1],\n",
    "    # [\"Gavet\", \"20230406-2101\", 1],\n",
    "    # [\"Gavet\", \"20230406-2121\", 1],\n",
    "    # [\"Gavet\", \"20230406-2143\", 1],\n",
    "    \n",
    "    # [\"Gavet\", \"20220609-2213\", 0],\n",
    "    # [\"Gavet\", \"20230330-2249\", 0],\n",
    "    # [\"Gavet\", \"20230330-2355\", 0],\n",
    "    # [\"Gavet\", \"20230331-0947\", 0],\n",
    "    # [\"Gavet\", \"20230331-1023\", 0],\n",
    "    # [\"Gavet\", \"20230331-1046\", 0],\n",
    "    # [\"Gavet\", \"20230331-1240\", 0],\n",
    "]\n",
    "\n",
    "save_disk = \"R:/\"\n",
    "save_true = False\n",
    "patch_per_image = 32\n",
    "\n",
    "for dataset_name, date_path, fold_id, in paths:\n",
    "    data_path = f\"./datasets/{dataset_name}/folds.json\"\n",
    "    train, test = load_dataset(data_path, True, as_numpy=False)[fold_id]\n",
    "    dataset_mask = DataIterator(\n",
    "        train, 32, patch_per_image=patch_per_image // 2, inv_values=False\n",
    "    ).get_dataset()\n",
    "    dataset_test = DataIterator(\n",
    "        train, 32, patch_per_image=patch_per_image, inv_values=False\n",
    "    ).get_dataset()\n",
    "\n",
    "    images, masks = dataset_test[1], dataset_mask[0]\n",
    "\n",
    "    validation_data = np.asarray(\n",
    "        [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in images],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "\n",
    "    true_path = os.path.join(\n",
    "        save_disk, f\"TF_LOGS/our/{dataset_name}/dataset_true_fold_{fold_id}/\"\n",
    "    )\n",
    "    for k in range(len(validation_data)):\n",
    "        Path(true_path).mkdir(exist_ok=True, parents=True)\n",
    "        io.imsave(\n",
    "            os.path.join(true_path, f\"{k}.png\"), validation_data[k], check_contrast=False\n",
    "        )\n",
    "\n",
    "    fids = []\n",
    "    preds_path = []\n",
    "    for i in tqdm(range(100)):\n",
    "        g_path = (\n",
    "            f\"F:/Deep Learning/GAN-CNN-CE/logs/{date_path}/generators/model_{i}.h5\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(g_path):\n",
    "            continue\n",
    "        try:\n",
    "            generator: Model = remove_gaussian_from_model(load_model(g_path, compile=False))\n",
    "        except:\n",
    "            if i == 0:\n",
    "                print('cannot remove gaussian')\n",
    "            generator: Model = load_model(g_path, compile=False)\n",
    "        \n",
    "        new_mask = generator.predict_on_batch(\n",
    "            [1-masks, np.random.normal(size=masks.shape)])\n",
    "        imgs = np.asarray(\n",
    "            [color.gray2rgb((im[..., 0] + 1) / 2 * 255) for im in new_mask],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        pred_path = os.path.join(\n",
    "            save_disk, f\"TF_LOGS/our/{dataset_name}/{date_path}/{i}/\")\n",
    "        preds_path.append(pred_path)\n",
    "        Path(pred_path).mkdir(exist_ok=True, parents=True)\n",
    "        for k in range(len(imgs)):\n",
    "            io.imsave(os.path.join(\n",
    "                pred_path, f\"{k}.png\"), imgs[k], check_contrast=False)\n",
    "\n",
    "    command = (\n",
    "        f\"python -m pytorch_fid {str(true_path)} {' '.join(preds_path)} --device cuda:0 --batch-size 128 --num-workers 0 --dims 2048\"\n",
    "    )\n",
    "    arch = subprocess.check_output(command, shell=True)\n",
    "\n",
    "    fids_str: str = re.findall(\"\\[(.*?)\\]\", str(arch))[0]\n",
    "    fids = [float(val) for val in fids_str.split(',')]\n",
    "    np.save(f\"ablation/{dataset_name}_{date_path}_{fold_id}.npy\", fids)\n",
    "    print(date_path, np.sort(fids)[:5], np.argsort(fids)[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablation\\Alizarine_20230410-2116_3.npy [57.07219175 58.64852124 65.25858369 67.64988741 71.32552164] [18 24 25 20 30]\n",
      "ablation\\Alizarine_20230410-2152_3.npy [40.48346294 44.1504507  46.49580349 47.94317385 49.28306935] [96 78 65 60 67]\n",
      "ablation\\Alizarine_20230410-2229_3.npy [53.84472006 56.20737058 57.44760235 63.2678175  66.37898473] [65 71 16 63 58]\n",
      "ablation\\Alizarine_20230410-2306_3.npy [53.60088737 53.86295804 61.31992451 62.08831634 65.60697053] [35  7  8 55 64]\n",
      "ablation\\Alizarine_20230410-2338_3.npy [39.45736176 45.55028612 45.7483596  46.04107393 46.97180857] [91 95 97 89 63]\n",
      "ablation\\Alizarine_20230411-0013_3.npy [58.53753137 65.78021973 66.42443562 67.64621495 79.1568123 ] [16 15 11 10  6]\n",
      "ablation\\Alizarine_20230411-2300_3.npy [39.93702997 44.59753127 45.04171563 48.35078644 53.75794512] [47 43 42 41 40]\n",
      "ablation\\Alizarine_20230412-0005_3.npy [47.36619009 49.37698269 52.30405712 54.93396577 57.29337334] [43 39 42 49 27]\n",
      "ablation\\Alizarine_20230412-0911_3.npy [75.73891031 83.68971065 84.08568789 85.31570024 88.88014644] [12 13 14  9 15]\n",
      "ablation\\Gavet_20230407-1804_3.npy [ 71.0798621   97.16938806  97.89381737 104.10165385 106.05262353] [17  8 26 57  6]\n",
      "ablation\\Gavet_20230407-1840_3.npy [ 84.61955379  87.51672886 103.46925348 104.23312129 110.3638115 ] [85 70 69 84 87]\n",
      "ablation\\Gavet_20230407-1915_3.npy [61.38051528 61.55897624 61.62199398 65.69796603 67.00002177] [48 50 75 53 42]\n",
      "ablation\\Gavet_20230407-1955_3.npy [ 87.0183185   94.10139196 101.00991244 102.1441308  104.2853553 ] [14 67 17  9 69]\n",
      "ablation\\Gavet_20230407-2028_3.npy [55.33505311 65.4641787  68.48645277 70.45527032 72.36631169] [30 47 59 55 42]\n",
      "ablation\\Gavet_20230407-2104_3.npy [113.8845788  115.76705082 116.83317904 119.25901564 121.68402236] [71 84 62 57 59]\n",
      "ablation\\Rotterdam_1000_20230408-0926_3.npy [ 99.98462819 126.05180994 127.89483722 154.5230667  171.53342805] [20 17  5 70 31]\n",
      "ablation\\Rotterdam_1000_20230408-1025_3.npy [112.84785202 122.3684606  131.1417847  137.10432287 141.54142356] [52  8 49 10 48]\n",
      "ablation\\Rotterdam_1000_20230408-1124_3.npy [114.47870366 125.31767138 125.79827823 127.88448793 134.40091213] [91  2 34 42 41]\n",
      "ablation\\Rotterdam_1000_20230408-1226_3.npy [140.69927948 160.05159916 160.07971548 166.76509286 173.0340097 ] [92 84 61 55 56]\n",
      "ablation\\Rotterdam_1000_20230408-1318_3.npy [134.99827381 139.93530297 152.50186911 152.94482703 161.11870304] [48 47 65 39 31]\n",
      "ablation\\Rotterdam_1000_20230408-1413_3.npy [112.35441641 121.48538748 134.97935582 136.32484737 145.55972829] [88 90 25 24 85]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nablation\\\\Alizarine_20230411-2300_3.npy [39.93702997 44.59753127 45.04171563 48.35078644 53.75794512] [47 43 42 41 40]\\nablation\\\\Alizarine_20230412-0005_3.npy [47.36619009 49.37698269 52.30405712 54.93396577 57.29337334] [43 39 42 49 27]\\nablation\\\\Alizarine_20230410-2229_3.npy [53.84472006 56.20737058 57.44760235 63.2678175  66.37898473] [65 71 16 63 58]\\nablation\\\\Alizarine_20230410-2306_3.npy [53.60088737 53.86295804 61.31992451 62.08831634 65.60697053] [35  7  8 55 64]\\nablation\\\\Alizarine_20230412-0911_3.npy [75.73891031 83.68971065 84.08568789 85.31570024 88.88014644] [12 13 14  9 15]\\nablation\\\\Alizarine_20230411-0013_3.npy [58.53753137 65.78021973 66.42443562 67.64621495 79.1568123 ] [16 15 11 10  6]\\n\\nablation\\\\Gavet_20230407-1804_3.npy [ 71.0798621   97.16938806  97.89381737 104.10165385 106.05262353] [17  8 26 57  6]\\nablation\\\\Gavet_20230407-1840_3.npy [ 84.61955379  87.51672886 103.46925348 104.23312129 110.3638115 ] [85 70 69 84 87]\\nablation\\\\Gavet_20230407-1915_3.npy [61.38051528 61.55897624 61.62199398 65.69796603 67.00002177] [48 50 75 53 42]\\nablation\\\\Gavet_20230407-1955_3.npy [ 87.0183185   94.10139196 101.00991244 102.1441308  104.2853553 ] [14 67 17  9 69]\\nablation\\\\Gavet_20230407-2028_3.npy [55.33505311 65.4641787  68.48645277 70.45527032 72.36631169] [30 47 59 55 42]\\nablation\\\\Gavet_20230407-2104_3.npy [113.8845788  115.76705082 116.83317904 119.25901564 121.68402236] [71 84 62 57 59]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "for p in glob('ablation/*'):\n",
    "    arr = np.load(p)\n",
    "    print(p, np.sort(arr)[:5], np.argsort(arr)[:5])\n",
    "\n",
    "\"\"\"\n",
    "ablation\\Alizarine_20230411-2300_3.npy [39.93702997 44.59753127 45.04171563 48.35078644 53.75794512] [47 43 42 41 40]\n",
    "ablation\\Alizarine_20230412-0005_3.npy [47.36619009 49.37698269 52.30405712 54.93396577 57.29337334] [43 39 42 49 27]\n",
    "ablation\\Alizarine_20230410-2229_3.npy [53.84472006 56.20737058 57.44760235 63.2678175  66.37898473] [65 71 16 63 58]\n",
    "ablation\\Alizarine_20230410-2306_3.npy [53.60088737 53.86295804 61.31992451 62.08831634 65.60697053] [35  7  8 55 64]\n",
    "ablation\\Alizarine_20230412-0911_3.npy [75.73891031 83.68971065 84.08568789 85.31570024 88.88014644] [12 13 14  9 15]\n",
    "ablation\\Alizarine_20230411-0013_3.npy [58.53753137 65.78021973 66.42443562 67.64621495 79.1568123 ] [16 15 11 10  6]\n",
    "\n",
    "ablation\\Gavet_20230407-1804_3.npy [ 71.0798621   97.16938806  97.89381737 104.10165385 106.05262353] [17  8 26 57  6]\n",
    "ablation\\Gavet_20230407-1840_3.npy [ 84.61955379  87.51672886 103.46925348 104.23312129 110.3638115 ] [85 70 69 84 87]\n",
    "ablation\\Gavet_20230412-1845_3.npy [ 93.99914365 126.15721594 130.16915836 138.72851774 143.11731833] [44 38 37 12 39]\n",
    "ablation\\Gavet_20230407-1955_3.npy [ 87.0183185   94.10139196 101.00991244 102.1441308  104.2853553 ] [14 67 17  9 69]\n",
    "ablation\\Gavet_20230412-1904_3.npy [ 63.7617624  76.76399027 84.39626781 92.12829262 97.53652036] [39 32 34 19 27]\n",
    "ablation\\Gavet_20230407-2104_3.npy [ 113.8845788  115.76705082 116.83317904 119.25901564 121.68402236] [71 84 62 57 59]\n",
    "\n",
    "\n",
    "ablation\\Rotterdam_1000_20230408-0926_3.npy [ 99.98462819 126.05180994 127.89483722 154.5230667  171.53342805] [20 17  5 70 31]\n",
    "ablation\\Rotterdam_1000_20230408-1025_3.npy [112.84785202 122.3684606  131.1417847  137.10432287 141.54142356] [52  8 49 10 48]\n",
    "ablation\\Rotterdam_1000_20230408-1124_3.npy [114.47870366 125.31767138 125.79827823 127.88448793 134.40091213] [91  2 34 42 41]\n",
    "ablation\\Rotterdam_1000_20230408-1226_3.npy [140.69927948 160.05159916 160.07971548 166.76509286 173.0340097 ] [92 84 61 55 56]\n",
    "ablation\\Rotterdam_1000_20230408-1318_3.npy [134.99827381 139.93530297 152.50186911 152.94482703 161.11870304] [48 47 65 39 31]\n",
    "ablation\\Rotterdam_1000_20230408-1413_3.npy [112.35441641 121.48538748 134.97935582 136.32484737 145.55972829] [88 90 25 24 85]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8a429dd3384afab0009274e63d060e112a5cb77c699fca471faa7f01f5c6825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
